{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file reads silver layer from adls gen2 and proccess it then save it as gold layer in aws db\n",
    "It should run once every x moment\n",
    "It is schudeled using databricks cron jobs jo run every hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475f7df4-b83e-45ef-9d0b-68448a14f8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ======= ADLS Gen2 Config =======\n",
    "storage_account = \"xxx\"\n",
    "container = \"xxx\" \n",
    "client_id = \"xxx\"\n",
    "tenant_id = \"xxx\"\n",
    "client_secret = \"xxx\"\n",
    "\n",
    "# ======= Spark Configuration for OAuth =======\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
    ")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "    f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    ")\n",
    "\n",
    "adls_base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/\"\n",
    "target_path = f\"{adls_base_path}gold/customers/\"\n",
    "checkpoint_path = f\"{adls_base_path}checkpoints/customers_delta/\"\n",
    "\n",
    "\n",
    "# ======= AWS Config =======\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://mlprojectdb.cx4k04y48cjc.eu-north-1.rds.amazonaws.com:5432/telco_churn\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"mlpipeline\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "silver_path = f\"{adls_base_path}/silver/customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b789a3bf-821f-41bc-b33b-eb7b1b6026b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\").orderBy(col(\"updated_at\").desc())\n",
    "\n",
    "gold_df = (\n",
    "    df\n",
    "    .withColumn(\"rn\", row_number().over(window))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"year\", \"month\", \"day\", \"hour\")\n",
    ")\n",
    "\n",
    "staging_table = \"customers_staging\"\n",
    "\n",
    "gold_df.write \\\n",
    "    .jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=staging_table,\n",
    "        mode=\"overwrite\",\n",
    "        properties=db_properties\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8d63f8-4f5c-4211-bddf-bff976ba0ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"event_time\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "\n",
    "existing_ids_df = spark.read.jdbc(jdbc_url, \"customers\", properties=db_properties).select(\"customer_id\")\n",
    "new_customers_only = gold_df.join(existing_ids_df, on=\"customer_id\", how=\"left_anti\")\n",
    "new_customers_only = new_customers_only.drop(*cols_to_drop)\n",
    "\n",
    "new_customers_only.write.mode(\"append\").jdbc(jdbc_url, \"customers\", properties=db_properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_postgres",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
